{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Gen AI APP Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"GROQ_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\" ## Tracing of Langchain\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Data Ingestion--From the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1d237f1f0e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/evaluation\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nEvaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate a complex agentTest a ReAct agent with Pytest/Vitest and LangSmithHow-to GuidesAnalyze a single experimentLog user feedback using the SDKHow to run an evaluationCreating and Managing Datasets in the UIRenaming an experimentHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyRunning an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application\\'s intermediate stepsHow to version a datasetUse annotation queuesDataset SharingHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to audit evaluator scoresHow to improve your evaluator with few-shot examplesHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to define a target function to evaluateHow to download experiment results as a CSVRun an evaluation with multimodal contentHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (Python only)How to return categorical vs numerical metricsHow to simulate multi-turn interactionsHow to return multiple scores in one evaluatorHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationEvaluation Quick Start\\nEvaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don\\'t always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\\nEvaluations are made up of three components:\\n\\nA dataset with test inputs and optionally expected outputs.\\nA target function that defines what you\\'re evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.\\nEvaluators that score your target function\\'s outputs.\\n\\nThis quick start guides you through running a simple evaluation to test the correctness of LLM responses with the LangSmith SDK or UI.\\nSDKUItipThis quickstart uses prebuilt LLM-as-judge evaluators from the open-source openevals package. OpenEvals includes a set of commonly used evaluators and is a great starting point if you\\'re new to evaluations.\\nIf you want greater flexibility in how you evaluate your apps, you can also define completely custom evaluators using your own code.1. Install Dependencies\\u200bPythonTypeScriptpip install -U langsmith openevals openainpm install langsmith openevals openaiinfoIf you are using yarn as your package manager, you will also need to manually install @langchain/core as a peer dependency of openevals. This is not required for LangSmith evals in general - you may define evaluators using arbitrary custom code.\\n2. Create a LangSmith API key\\u200bTo create an API key, head to the Settings page. Then click Create API Key.3. Set up your environment\\u200bBecause this quickstart uses OpenAI models, you\\'ll need to set the OPENAI_API_KEY environment variable as well as the\\nrequired LangSmith ones:Shellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\"<your-langchain-api-key>\"# This example uses OpenAI, but you can use other LLM providers if desiredexport OPENAI_API_KEY=\"<your-openai-api-key>\"4. Create a dataset\\u200bNext, define example input and reference output pairs that you\\'ll use to evaluate your app:PythonTypeScriptfrom langsmith import Clientclient = Client()# Programmatically create a dataset in LangSmith# For other dataset creation methods, see:# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_applicationdataset = client.create_dataset(    dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\")# Create examplesexamples = [    {        \"inputs\": {\"question\": \"Which country is Mount Kilimanjaro located in?\"},        \"outputs\": {\"answer\": \"Mount Kilimanjaro is located in Tanzania.\"},    },    {        \"inputs\": {\"question\": \"What is Earth\\'s lowest point?\"},        \"outputs\": {\"answer\": \"Earth\\'s lowest point is The Dead Sea.\"},    },]# Add examples to the datasetclient.create_examples(dataset_id=dataset.id, examples=examples)import { Client } from \"langsmith\";const client = new Client();// Programmatically create a dataset in LangSmith// For other dataset creation methods, see:// https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically// https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_applicationconst dataset = await client.createDataset(\"Sample dataset\", {  description: \"A sample dataset in LangSmith.\",});// Create inputs and reference outputsconst examples = [  {    inputs: { question: \"Which country is Mount Kilimanjaro located in?\" },    outputs: { answer: \"Mount Kilimanjaro is located in Tanzania.\" },    dataset_id: dataset.id,  },  {    inputs: { question: \"What is Earth\\'s lowest point?\" },    outputs: { answer: \"Earth\\'s lowest point is The Dead Sea.\" },    dataset_id: dataset.id,  },];// Add examples to the datasetawait client.createExamples(examples);5. Define what you\\'re evaluating\\u200bNow, define target function that contains what you\\'re evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.PythonTypeScriptfrom langsmith import wrappersfrom openai import OpenAI# Wrap the OpenAI client for LangSmith tracingopenai_client = wrappers.wrap_openai(OpenAI())      # Define the application logic you want to evaluate inside a target function# The SDK will automatically send the inputs from the dataset to your target functiondef target(inputs: dict) -> dict:    response = openai_client.chat.completions.create(        model=\"gpt-4o-mini\",        messages=[            {\"role\": \"system\", \"content\": \"Answer the following question accurately\"},            {\"role\": \"user\", \"content\": inputs[\"question\"]},        ],    )    return { \"answer\": response.choices[0].message.content.strip() }import { wrapOpenAI } from \"langsmith/wrappers\";import OpenAI from \"openai\";const openai = wrapOpenAI(new OpenAI());// Define the application logic you want to evaluate inside a target function// The SDK will automatically send the inputs from the dataset to your target functionasync function target(inputs: { question: string }): Promise<{ answer: string }> {  const response = await openai.chat.completions.create({    model: \"gpt-4o-mini\",    messages: [      { role: \"system\", content: \"Answer the following question accurately\" },      { role: \"user\", content: inputs.question },    ],  });  return { answer: response.choices[0].message.content?.trim() || \"\" };}6. Define evaluator\\u200bImport a prebuilt prompt from openevals and create an evaluator.\\noutputs are the result of your target function. reference_outputs / referenceOutputs are from the example pairs you defined in step 4 above.infoCORRECTNESS_PROMPT is just an f-string with variables for \"inputs\", \"outputs\", and \"reference_outputs\".\\nSee here for more information on customizing OpenEvals prompts.PythonTypeScriptfrom openevals.llm import create_llm_as_judgefrom openevals.prompts import CORRECTNESS_PROMPTdef correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):    evaluator = create_llm_as_judge(        prompt=CORRECTNESS_PROMPT,        model=\"openai:o3-mini\",        feedback_key=\"correctness\",    )    eval_result = evaluator(        inputs=inputs,        outputs=outputs,        reference_outputs=reference_outputs    )    return eval_resultimport { createLLMAsJudge, CORRECTNESS_PROMPT } from \"openevals\";const correctnessEvaluator = async (params: {  inputs: Record<string, unknown>;  outputs: Record<string, unknown>;  referenceOutputs?: Record<string, unknown>;}) => {  const evaluator = createLLMAsJudge({    prompt: CORRECTNESS_PROMPT,    model: \"openai:o3-mini\",    feedbackKey: \"correctness\",  });  const evaluatorResult = await evaluator({    inputs: params.inputs,    outputs: params.outputs,    referenceOutputs: params.referenceOutputs,  });  return evaluatorResult;};7. Run and view results\\u200bFinally, run the experiment!PythonTypeScript# After running the evaluation, a link will be provided to view the results in langsmithexperiment_results = client.evaluate(    target,    data=\"Sample dataset\",    evaluators=[        correctness_evaluator,        # can add multiple evaluators here    ],    experiment_prefix=\"first-eval-in-langsmith\",    max_concurrency=2,)import { evaluate } from \"langsmith/evaluation\";// After running the evaluation, a link will be provided to view the results in langsmithawait evaluate(  target,  {    data: \"Sample dataset\",    evaluators: [      correctnessEvaluator,      // can add multiple evaluators here    ],    experimentPrefix: \"first-eval-in-langsmith\",    maxConcurrency: 2,  });Click the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of the experiment.Next steps\\u200btipTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.\\nCheck out the OpenEvals README to see all available prebuilt evaluators and how to customize them.\\nLearn how to define custom evaluators that contain arbitrary code.\\nSee the How-to guides for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.\\nFor end-to-end walkthroughs see Tutorials.\\nFor comprehensive descriptions of every class and function see the API reference.\\nOr, if you prefer video tutorials, check out the Datasets, Evaluators, and Experiments videos from the Introduction to LangSmith Course.1. Navigate to the Playground\\u200bLangSmith\\'s Prompt Playground makes it possible to run evaluations over different prompts, new models or test different model configurations. Go to LangSmith\\'s Playground in the UI.2. Create a prompt\\u200bModify the system prompt to:Answer the following question accurately:3. Create a dataset\\u200bClick Set up Evaluation, then use the + New button in the dropdown to create a new dataset.Add the following examples to the dataset:InputsReference Outputsquestion: Which country is Mount Kilimanjaro located in?output: Mount Kilimanjaro is located in Tanzania.question: What is Earth\\'s lowest point?output: Earth\\'s lowest point is The Dead Sea.Press Save to save your newly created dataset.4. Add an evaluator\\u200bClick +Evaluator. Select Correctness from the pre-built evaluator options. Press Save.5. Run your evaluation\\u200bPress Start on the top right to run your evaluation. Running this evaluation will create an experiment that you can view in full by clicking the experiment name.Next steps\\u200btipTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.See the How-to guides for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.\\nLearn how to create and manage datasets in the UI\\nLearn how to run an evaluation from the prompt playground\\nIf you prefer video tutorials, check out the Playground videos from the Introduction to LangSmith Course.Was this page helpful?You can leave detailed feedback on GitHub.PreviousConceptual GuideNextQuick StartCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data--> Docs-->Divide our Docuemnts into chunks dcouments-->text-->vectors-->Vector Embeddings--->Vector Store DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate a complex agentTest a ReAct agent with Pytest/Vitest and LangSmithHow-to GuidesAnalyze a single experimentLog user feedback using the SDKHow to run an evaluationCreating and Managing Datasets in the UIRenaming an experimentHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyRunning an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application's intermediate stepsHow to version a datasetUse annotation queuesDataset SharingHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='(Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to audit evaluator scoresHow to improve your evaluator with few-shot examplesHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to define a target function to evaluateHow to download experiment results as a CSVRun an evaluation with multimodal contentHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (Python only)How to return categorical vs numerical'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='evaluatorHow to run an evaluation locally (Python only)How to return categorical vs numerical metricsHow to simulate multi-turn interactionsHow to return multiple scores in one evaluatorHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationEvaluation Quick Start'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\\nEvaluations are made up of three components:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"A dataset with test inputs and optionally expected outputs.\\nA target function that defines what you're evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.\\nEvaluators that score your target function's outputs.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"This quick start guides you through running a simple evaluation to test the correctness of LLM responses with the LangSmith SDK or UI.\\nSDKUItipThis quickstart uses prebuilt LLM-as-judge evaluators from the open-source openevals package. OpenEvals includes a set of commonly used evaluators and is a great starting point if you're new to evaluations.\\nIf you want greater flexibility in how you evaluate your apps, you can also define completely custom evaluators using your own code.1. Install Dependencies\\u200bPythonTypeScriptpip install -U langsmith openevals openainpm install langsmith openevals openaiinfoIf you are using yarn as your package manager, you will also need to manually install @langchain/core as a peer dependency of openevals. This is not required for LangSmith evals in general - you may define evaluators using arbitrary custom code.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"2. Create a LangSmith API key\\u200bTo create an API key, head to the Settings page. Then click Create API Key.3. Set up your environment\\u200bBecause this quickstart uses OpenAI models, you'll need to set the OPENAI_API_KEY environment variable as well as the\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='required LangSmith ones:Shellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\"<your-langchain-api-key>\"# This example uses OpenAI, but you can use other LLM providers if desiredexport OPENAI_API_KEY=\"<your-openai-api-key>\"4. Create a dataset\\u200bNext, define example input and reference output pairs that you\\'ll use to evaluate your app:PythonTypeScriptfrom langsmith import Clientclient = Client()# Programmatically create a dataset in LangSmith# For other dataset creation methods, see:# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_applicationdataset = client.create_dataset(    dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\")# Create examplesexamples = [    {        \"inputs\": {\"question\": \"Which country is Mount Kilimanjaro located in?\"},        \"outputs\": {\"answer\": \"Mount Kilimanjaro is located in Tanzania.\"},    },    {'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='in?\"},        \"outputs\": {\"answer\": \"Mount Kilimanjaro is located in Tanzania.\"},    },    {        \"inputs\": {\"question\": \"What is Earth\\'s lowest point?\"},        \"outputs\": {\"answer\": \"Earth\\'s lowest point is The Dead Sea.\"},    },]# Add examples to the datasetclient.create_examples(dataset_id=dataset.id, examples=examples)import { Client } from \"langsmith\";const client = new Client();// Programmatically create a dataset in LangSmith// For other dataset creation methods, see:// https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically// https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_applicationconst dataset = await client.createDataset(\"Sample dataset\", {  description: \"A sample dataset in LangSmith.\",});// Create inputs and reference outputsconst examples = [  {    inputs: { question: \"Which country is Mount Kilimanjaro located in?\" },    outputs: { answer: \"Mount Kilimanjaro is located in Tanzania.\" },    dataset_id:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='located in?\" },    outputs: { answer: \"Mount Kilimanjaro is located in Tanzania.\" },    dataset_id: dataset.id,  },  {    inputs: { question: \"What is Earth\\'s lowest point?\" },    outputs: { answer: \"Earth\\'s lowest point is The Dead Sea.\" },    dataset_id: dataset.id,  },];// Add examples to the datasetawait client.createExamples(examples);5. Define what you\\'re evaluating\\u200bNow, define target function that contains what you\\'re evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.PythonTypeScriptfrom langsmith import wrappersfrom openai import OpenAI# Wrap the OpenAI client for LangSmith tracingopenai_client = wrappers.wrap_openai(OpenAI())      # Define the application logic you want to evaluate inside a target function# The SDK will automatically send the inputs from the dataset to your target functiondef target(inputs: dict) -> dict:    response = openai_client.chat.completions.create('),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='functiondef target(inputs: dict) -> dict:    response = openai_client.chat.completions.create(        model=\"gpt-4o-mini\",        messages=[            {\"role\": \"system\", \"content\": \"Answer the following question accurately\"},            {\"role\": \"user\", \"content\": inputs[\"question\"]},        ],    )    return { \"answer\": response.choices[0].message.content.strip() }import { wrapOpenAI } from \"langsmith/wrappers\";import OpenAI from \"openai\";const openai = wrapOpenAI(new OpenAI());// Define the application logic you want to evaluate inside a target function// The SDK will automatically send the inputs from the dataset to your target functionasync function target(inputs: { question: string }): Promise<{ answer: string }> {  const response = await openai.chat.completions.create({    model: \"gpt-4o-mini\",    messages: [      { role: \"system\", content: \"Answer the following question accurately\" },      { role: \"user\", content: inputs.question },    ],  });  return { answer:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='accurately\" },      { role: \"user\", content: inputs.question },    ],  });  return { answer: response.choices[0].message.content?.trim() || \"\" };}6. Define evaluator\\u200bImport a prebuilt prompt from openevals and create an evaluator.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='outputs are the result of your target function. reference_outputs / referenceOutputs are from the example pairs you defined in step 4 above.infoCORRECTNESS_PROMPT is just an f-string with variables for \"inputs\", \"outputs\", and \"reference_outputs\".'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='See here for more information on customizing OpenEvals prompts.PythonTypeScriptfrom openevals.llm import create_llm_as_judgefrom openevals.prompts import CORRECTNESS_PROMPTdef correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):    evaluator = create_llm_as_judge(        prompt=CORRECTNESS_PROMPT,        model=\"openai:o3-mini\",        feedback_key=\"correctness\",    )    eval_result = evaluator(        inputs=inputs,        outputs=outputs,        reference_outputs=reference_outputs    )    return eval_resultimport { createLLMAsJudge, CORRECTNESS_PROMPT } from \"openevals\";const correctnessEvaluator = async (params: {  inputs: Record<string, unknown>;  outputs: Record<string, unknown>;  referenceOutputs?: Record<string, unknown>;}) => {  const evaluator = createLLMAsJudge({    prompt: CORRECTNESS_PROMPT,    model: \"openai:o3-mini\",    feedbackKey: \"correctness\",  });  const evaluatorResult = await evaluator({    inputs: params.inputs,    outputs: params.outputs,'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='const evaluatorResult = await evaluator({    inputs: params.inputs,    outputs: params.outputs,    referenceOutputs: params.referenceOutputs,  });  return evaluatorResult;};7. Run and view results\\u200bFinally, run the experiment!PythonTypeScript# After running the evaluation, a link will be provided to view the results in langsmithexperiment_results = client.evaluate(    target,    data=\"Sample dataset\",    evaluators=[        correctness_evaluator,        # can add multiple evaluators here    ],    experiment_prefix=\"first-eval-in-langsmith\",    max_concurrency=2,)import { evaluate } from \"langsmith/evaluation\";// After running the evaluation, a link will be provided to view the results in langsmithawait evaluate(  target,  {    data: \"Sample dataset\",    evaluators: [      correctnessEvaluator,      // can add multiple evaluators here    ],    experimentPrefix: \"first-eval-in-langsmith\",    maxConcurrency: 2,  });Click the link printed out by your evaluation run to access the LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='maxConcurrency: 2,  });Click the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of the experiment.Next steps\\u200btipTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='Check out the OpenEvals README to see all available prebuilt evaluators and how to customize them.\\nLearn how to define custom evaluators that contain arbitrary code.\\nSee the How-to guides for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.\\nFor end-to-end walkthroughs see Tutorials.\\nFor comprehensive descriptions of every class and function see the API reference.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"Or, if you prefer video tutorials, check out the Datasets, Evaluators, and Experiments videos from the Introduction to LangSmith Course.1. Navigate to the Playground\\u200bLangSmith's Prompt Playground makes it possible to run evaluations over different prompts, new models or test different model configurations. Go to LangSmith's Playground in the UI.2. Create a prompt\\u200bModify the system prompt to:Answer the following question accurately:3. Create a dataset\\u200bClick Set up Evaluation, then use the + New button in the dropdown to create a new dataset.Add the following examples to the dataset:InputsReference Outputsquestion: Which country is Mount Kilimanjaro located in?output: Mount Kilimanjaro is located in Tanzania.question: What is Earth's lowest point?output: Earth's lowest point is The Dead Sea.Press Save to save your newly created dataset.4. Add an evaluator\\u200bClick +Evaluator. Select Correctness from the pre-built evaluator options. Press Save.5. Run your evaluation\\u200bPress Start on the top\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='from the pre-built evaluator options. Press Save.5. Run your evaluation\\u200bPress Start on the top right to run your evaluation. Running this evaluation will create an experiment that you can view in full by clicking the experiment name.Next steps\\u200btipTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.See the How-to guides for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='Learn how to create and manage datasets in the UI\\nLearn how to run an evaluation from the prompt playground\\nIf you prefer video tutorials, check out the Playground videos from the Introduction to LangSmith Course.Was this page helpful?You can leave detailed feedback on GitHub.PreviousConceptual GuideNextQuick StartCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aswan\\AGen ai\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb=FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1d207eada90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\\nEvaluations are made up of three components:\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query From a vector db\n",
    "query=\"Evaluations are a quantitative way to measure\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001D22C262120> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001D22C262BA0> root_client=<openai.OpenAI object at 0x000001D22B077230> root_async_client=<openai.AsyncOpenAI object at 0x000001D22C262900> model_name='llama3-8b-8192' model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"llama3-8b-8192\",\n",
    "               openai_api_base=\"https://api.groq.com/openai/v1\",\n",
    "               openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "               )\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001D22C262120>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001D22C262BA0>, root_client=<openai.OpenAI object at 0x000001D22B077230>, root_async_client=<openai.AsyncOpenAI object at 0x000001D22C262900>, model_name='llama3-8b-8192', model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the primary purpose of evaluations in Large Language Model (LLM) applications?\\n\\nAnswer: Evaluations are a quantitative way to measure the performance of LLM applications, providing a structured way to identify failures, compare changes across different versions, and build more reliable AI applications.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\" \",\n",
    "    \"context\":[Document(page_content=\"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just set up. That way, we can use the retriever to dynamically select the most relevant documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1d207eada90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input--->Retriever--->vectorstoredb\n",
    "\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D207EADA90>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001D22C262120>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001D22C262BA0>, root_client=<openai.OpenAI object at 0x000001D22B077230>, root_async_client=<openai.AsyncOpenAI object at 0x000001D22C262900>, model_name='llama3-8b-8192', model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the key to defining custom evaluators for LLM applications?\\n\\nAccording to the context, the answer is: \"you can define completely custom evaluators using your own code.\"'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response form the LLM\n",
    "response=retrieval_chain.invoke({\"input\":\"Evaluations are a quantitative way to measure performance of LLM applications\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Evaluations are a quantitative way to measure performance of LLM applications',\n",
       " 'context': [Document(id='dcf53db1-dc48-452e-9fa5-4f0256b8b095', metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\\nEvaluations are made up of three components:\"),\n",
       "  Document(id='15c02d4b-1684-4522-9465-a61156398f57', metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"This quick start guides you through running a simple evaluation to test the correctness of LLM responses with the LangSmith SDK or UI.\\nSDKUItipThis quickstart uses prebuilt LLM-as-judge evaluators from the open-source openevals package. OpenEvals includes a set of commonly used evaluators and is a great starting point if you're new to evaluations.\\nIf you want greater flexibility in how you evaluate your apps, you can also define completely custom evaluators using your own code.1. Install Dependencies\\u200bPythonTypeScriptpip install -U langsmith openevals openainpm install langsmith openevals openaiinfoIf you are using yarn as your package manager, you will also need to manually install @langchain/core as a peer dependency of openevals. This is not required for LangSmith evals in general - you may define evaluators using arbitrary custom code.\"),\n",
       "  Document(id='090161be-bb61-45d7-864e-d4b68f25128f', metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"A dataset with test inputs and optionally expected outputs.\\nA target function that defines what you're evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.\\nEvaluators that score your target function's outputs.\"),\n",
       "  Document(id='3b04a3e9-2308-4ba1-bc55-1d37bb745914', metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='(Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to audit evaluator scoresHow to improve your evaluator with few-shot examplesHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to define a target function to evaluateHow to download experiment results as a CSVRun an evaluation with multimodal contentHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (Python only)How to return categorical vs numerical')],\n",
       " 'answer': 'What is the key to defining custom evaluators for LLM applications?\\n\\nAccording to the context, the answer is: \"you can define completely custom evaluators using your own code.\"'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='dcf53db1-dc48-452e-9fa5-4f0256b8b095', metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\\nEvaluations are made up of three components:\"),\n",
       " Document(id='15c02d4b-1684-4522-9465-a61156398f57', metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"This quick start guides you through running a simple evaluation to test the correctness of LLM responses with the LangSmith SDK or UI.\\nSDKUItipThis quickstart uses prebuilt LLM-as-judge evaluators from the open-source openevals package. OpenEvals includes a set of commonly used evaluators and is a great starting point if you're new to evaluations.\\nIf you want greater flexibility in how you evaluate your apps, you can also define completely custom evaluators using your own code.1. Install Dependencies\\u200bPythonTypeScriptpip install -U langsmith openevals openainpm install langsmith openevals openaiinfoIf you are using yarn as your package manager, you will also need to manually install @langchain/core as a peer dependency of openevals. This is not required for LangSmith evals in general - you may define evaluators using arbitrary custom code.\"),\n",
       " Document(id='090161be-bb61-45d7-864e-d4b68f25128f', metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"A dataset with test inputs and optionally expected outputs.\\nA target function that defines what you're evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.\\nEvaluators that score your target function's outputs.\"),\n",
       " Document(id='3b04a3e9-2308-4ba1-bc55-1d37bb745914', metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='(Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to audit evaluator scoresHow to improve your evaluator with few-shot examplesHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to define a target function to evaluateHow to download experiment results as a CSVRun an evaluation with multimodal contentHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (Python only)How to return categorical vs numerical')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
